{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1º Trabalho prático NLP/DCC \n",
    "### Como avaliar de forma intrínseca um modelo de linguagem.\n",
    "Nome: Welton Augusto Rodrigues Santos\n",
    "\n",
    "## Introdução\n",
    "\n",
    "A representação vetorial de documentos desempenha um papel crucial em diversas áreas de NLP. Produzir boas representações vetoriais é um tarefa amplamente estudada na literatura. Entre as diversas estratégias existentes, a abordagem que mais se destaca consiste na representação de palavras e documentos por meio de vetores densos (word embeddings) produzidos por modelos neurais profundos. Nesta linha, framewords como Glove, Word2Vec e Fasttext são amplamente utilizados, principalmente por oferecer boas representações vetoriais com informação semânticas com custo computacional consideravelmente baixo.\n",
    "\n",
    "Neste trabalho, temos como objetivo avaliar o desempenho do framewor Word2Vec intrínsecamente. Para tal avaliação produziremos diferentes modelos para conjuntos de combinaçẽos sobre os parâmetros: tamanho do vetor (vector_size), tamanho da janela (window_size), modelo de produção vetorial (Skip-Gram ou CBOW) e frequência mínima de palavras. O corpus utilizado para treinar o modelo consiste no corpus Text8, que contem os 100 primeiros MB do Wikipedia. Utilizamos para avaliação intrínseca a base de teste question words utilizado no trabalho (Efficient Estimation of Word Representations in Vector Space, Mikolov et al.). Nesta tarefa combinamos linearmente três palavras relacionadas semanticamente e com o resultado da combinação (vetor denso gerado) nós fazemos uma query a partir da similaridade de cosseno e recuperamos o ranking das cinco palavras mais próximas do vetor resultante. Ex. da query ( king - man + woman = queen. Como forma de atribuir um score para o algoritmo para medir seu desempenho, definimos quatro métricas: \n",
    "* hits: quantidade de vezes que a primeira palavra do ranking retornado pela query é a palavra esperada;\n",
    "* miss: quantidade de vezes que uma query foi feita e a palavra esperada não está no ranking (não estiver entre as cinco primeiras palavras mais próximas do vetor resultante);\n",
    "* score: pontuação que relaciona a distância entre o vetor resultante da palavra esperada no ranking, definida como se segue: 1 / ri * si. ri é a posição da palavra esperada no ranking e w é a similaridade entre o vetor resultante e a palavra esperada.\n",
    "Para medir os acertos e erros do modelos, utilizamos \n",
    "* close_to: quantidade de vezes que a palavra esperada apareceu no ranking.\n",
    "\n",
    "Neste jupyter notebook, apresentamos o código utilizado para treinar os modelos com os diferentes parâmetros e também o código para medir o desempenho dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import json\n",
    "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
    "from gensim.models import KeyedVectors\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(stop_path=\"data/stop_words_english.txt\"):\n",
    "\n",
    "    stop_words = {}\n",
    "    with open(stop_path, 'r') as fd:\n",
    "        word_list = fd.read().splitlines()\n",
    "        word_list.pop()\n",
    "        for word in word_list:\n",
    "            stop_words[word] = True\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def get_text(text_clean_path = \"data/text_clean\", text_raw_path = \"data/text8\", chunk_size=1000):\n",
    "    text = None\n",
    "    stop_words = load_stop_words()\n",
    "    # Se o dataset já existir.\n",
    "    if os.path.exists(text_clean_path):\n",
    "        with open(text_clean_path, 'r') as fd:\n",
    "            text = [fd.read().split(' ')]\n",
    "            return text\n",
    "    # Se não, pré processa ele.\n",
    "    else:\n",
    "        docs = []\n",
    "        with open(text_raw_path, 'r') as fd:\n",
    "            sentences = fd.read().split('\\n')\n",
    "            for sent in sentences:\n",
    "                clean_sent = sent.translate(str.maketrans('','', string.punctuation)).split(' ')\n",
    "                # Quebrando os documentos em chunks de 1000 palavras.\n",
    "                cont = 0\n",
    "                while cont < len(clean_sent):\n",
    "                    chunck = []\n",
    "                    # Para cada chunk.\n",
    "                    for word in clean_sent[cont : cont + chunk_size]:\n",
    "                        # Verifica se a palavra é stopword.\n",
    "                        if word not in stop_words:\n",
    "                            chunck.append(word)\n",
    "                    cont += chunk_size\n",
    "                    docs.append(chunck)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17006"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = get_text()\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(text, window, vsize, sg, min_count):\n",
    "    model_path = f\"models/window_size-{window}_vector_size-{vsize}_sg-{sg}_min_count-{min_count}\"\n",
    "    model = Word2Vec(sentences=text, vector_size=vsize, window=window, min_count=min_count, sg=sg, workers=10)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [1,3,5,7,9,11,13,15,19,23,25]\n",
    "vector_sizes = [50, 75, 100, 150, 200, 300, 500, 1000, 2000]\n",
    "min_counts = [1, 2, 3, 5, 7, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag de controle para rodar os modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mude a flag TRAIN_MODELS para true somente se quiser\n",
    "treinar os modelos.\n",
    "\"\"\"\n",
    "\n",
    "TRAIN_MODELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODELS:\n",
    "    \n",
    "    # Variando o tamanho da janela.\n",
    "    for window in tqdm(window_sizes):\n",
    "        train_model(sentences, window, 100, 1, 1)\n",
    "    \n",
    "    # Variando o tamanho do vetor.\n",
    "    for vsize in tqdm(vector_sizes):\n",
    "        train_model(sentences, 5, vsize, 1, 1)\n",
    "    \n",
    "    # Variando o modelo (skip or cbow)\n",
    "    for sg in tqdm([0,1]):\n",
    "        train_model(sentences, 5, 100, sg, 1)\n",
    "    \n",
    "    # Variando o min_count.\n",
    "    for min_count in tqdm(min_counts):\n",
    "        train_model(sentences, 5, 100, 1, min_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_questions(qpath=\"data/questions-words.txt\"):\n",
    "\n",
    "    dquestions = {}\n",
    "    with open(qpath, 'r') as fd:\n",
    "        key = None\n",
    "        tests = fd.read().lower().split('\\n')\n",
    "        rest = tests.pop()\n",
    "        for test in tests:\n",
    "            categ = test[0]\n",
    "            if categ == ':':\n",
    "                dquestions[test] = []\n",
    "                key = test\n",
    "            else:\n",
    "                dquestions[key].append(test.split(' '))\n",
    "    return dquestions\n",
    "\n",
    "def are_words_in(words, model_words):\n",
    "\n",
    "    for word in words:\n",
    "        if word not in model_words.wv.key_to_index:\n",
    "            print(f\"{word} does not exist\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "def ranking_words_most_similar(words, model, topn=5):\n",
    "    \n",
    "    try:\n",
    "        w1, w2, w3, _ = words\n",
    "        return list(model.wv.most_similar(positive=[w1, w3], negative=[w2], topn=topn))\n",
    "    except Exception as err:\n",
    "        return []\n",
    "\n",
    "\n",
    "def evaluate_model(dquestions, model, topn=5):\n",
    "    \n",
    "    score_categ = {}\n",
    "    number_exceptions = 0\n",
    "    # Para cada categoria de teste.\n",
    "    for categ in dquestions:\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        close_to = 0\n",
    "        score = 0\n",
    "        score_categ[categ] = {}\n",
    "        local_exceptions = 0\n",
    "        # Para cada test.\n",
    "        for test in dquestions[categ]:\n",
    "            target = test[-1]\n",
    "            ranking = ranking_words_most_similar(test, model, topn=topn)\n",
    "            if ranking:\n",
    "                #random_word = np.random.randint(len(ranking))\n",
    "                #target = ranking[random_word][0]\n",
    "                #if np.random.randint(2) == 0:\n",
    "                #    target = test[-1]\n",
    "                first_word = ranking.pop(0)\n",
    "                # Verificando se a primeira palavra é o target esperado.\n",
    "                if first_word[0] == target:\n",
    "                    hits += 1\n",
    "                    score += first_word[1]\n",
    "                # Se não verifique se a palavra está no ranking.\n",
    "                else:\n",
    "                    rwords = [w[0] for w in ranking ]\n",
    "                    if target in rwords:\n",
    "                        close_to += 1\n",
    "                        distance = rwords.index(target) + 1\n",
    "                        score += (1 / distance) * ranking[distance - 1][1]\n",
    "                    else:\n",
    "                        misses += 1\n",
    "            else:\n",
    "                number_exceptions +=1\n",
    "                local_exceptions += 1\n",
    "        score_categ[categ][\"hits\"] = hits\n",
    "        score_categ[categ][\"misses\"] = misses + local_exceptions\n",
    "        score_categ[categ][\"close_to\"] = close_to\n",
    "        score_categ[categ][\"score\"] = score\n",
    "    print(f\"Exceptions: {number_exceptions}\")\n",
    "    return score_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: window_size-5_vector_size-2000_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-2\n",
      "Exceptions: 3195\n",
      "\n",
      "Model: window_size-5_vector_size-50_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-1000_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-0_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-25_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-15_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-200_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-10\n",
      "Exceptions: 5562\n",
      "\n",
      "Model: window_size-3_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-7\n",
      "Exceptions: 4698\n",
      "\n",
      "Model: window_size-5_vector_size-500_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-7_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-75_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-13_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-150_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-9_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-11_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-5\n",
      "Exceptions: 3932\n",
      "\n",
      "Model: window_size-1_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-19_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-3\n",
      "Exceptions: 3659\n",
      "\n",
      "Model: window_size-5_vector_size-300_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-5_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n",
      "\n",
      "Model: window_size-23_vector_size-100_sg-1_min_count-1\n",
      "Exceptions: 2878\n"
     ]
    }
   ],
   "source": [
    "dquestions = prep_questions()\n",
    "#models_paths = np.random.choice([ f for f in os.listdir(\"models/\") if f.find(\"npy\") == -1 ], 3)\n",
    "models_paths = [ f for f in os.listdir(\"models/\") if f.find(\"npy\") == -1 ]\n",
    "models_score = {}\n",
    "for f in models_paths:\n",
    "    print(f\"\\nModel: {f}\")\n",
    "    model = KeyedVectors.load(f\"models/{f}\")\n",
    "    models_score[f] = evaluate_model(dquestions, model)\n",
    "    output = f\"outputs/models/{f}.json\"\n",
    "    with open(output, 'w') as fd:\n",
    "        json.dump(models_score[f], fd, indent=6)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a42a9c5247d45854756dcdfa5bcaf448c71a6cb5a74d180ad5b016b0912211a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
